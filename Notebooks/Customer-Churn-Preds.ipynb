{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "782a07bf-08de-4030-88e1-6731c4ac956e"
    }
   },
   "source": [
    "## Customer Churn Preds .. Imbalaced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario: Imagine you have trained and fine-tuned your model and used it to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "652b58d4-3b75-405f-9f11-24d0cd1f9656"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "### Customers that belong to class 0 are normal\n",
    "### Customers that belong to class 1 require follow up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "d6ff2283-cb13-468f-b0cc-0aefeab7b57f"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('churn_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a195ae30-1962-4427-859b-73a013dc10d6"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Actual'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classifier Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "# Explicitly stating labels. Pass=1, Fail=0\n",
    "def true_positive(y_true, y_pred): \n",
    "    return confusion_matrix(y_true, y_pred,labels=[1,0])[0, 0]\n",
    "\n",
    "def true_negative(y_true, y_pred): \n",
    "    return confusion_matrix(y_true,y_pred,labels=[1,0])[1, 1]\n",
    "\n",
    "def false_positive(y_true, y_pred): \n",
    "    return confusion_matrix(y_true, y_pred,labels=[1,0])[1, 0]\n",
    "\n",
    "def false_negative(y_true, y_pred): \n",
    "    return confusion_matrix(y_true, y_pred,labels=[1,0])[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Binary Classifier Metrics\n",
    "# Returns a dictionary {\"MetricName\":Value,...}\n",
    "\n",
    "def binary_classifier_metrics(y_true, y_pred):\n",
    "    metrics = {}\n",
    "\n",
    "    # References: \n",
    "    #  https://docs.aws.amazon.com/machine-learning/latest/dg/binary-classification.html\n",
    "    #  https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "    \n",
    "    # Definition:\n",
    "    # true positive = tp = how many samples were correctly classified as positive (count)\n",
    "    # true negative = tn = how many samples were correctly classified as negative (count)\n",
    "    # false positive = fp = how many negative samples were mis-classified as positive (count)\n",
    "    # false_negative = fn = how many positive samples were mis-classified as negative (count)\n",
    "    \n",
    "    # positive = number of positive samples (count)\n",
    "    #          = true positive + false negative\n",
    "    # negative = number of negative samples (count)\n",
    "    #          = true negative + false positive\n",
    "    \n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    tn = true_negative(y_true, y_pred)\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "    fn = false_negative(y_true, y_pred)\n",
    "    \n",
    "    positive = tp + fn\n",
    "    negative = tn + fp\n",
    "    \n",
    "    metrics['TruePositive'] = tp\n",
    "    metrics['TrueNegative'] = tn\n",
    "    metrics['FalsePositive'] = fp\n",
    "    metrics['FalseNegative'] = fn\n",
    "    \n",
    "    metrics['Positive'] = positive\n",
    "    metrics['Negative'] = negative\n",
    "    \n",
    "    # True Positive Rate (TPR, Recall) = true positive/positive\n",
    "    # How many positives were correctly classified? (fraction)\n",
    "    # Recall value closer to 1 is better. closer to 0 is worse\n",
    "    if tp == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = tp/positive\n",
    "        \n",
    "    metrics['Recall'] = recall\n",
    "    \n",
    "    # True Negative Rate = True Negative/negative\n",
    "    # How many negatives were correctly classified? (fraction)\n",
    "    # True Negative Rate value closer to 1 is better. closer to 0 is worse\n",
    "    if tn == 0:\n",
    "        tnr = 0\n",
    "    else:\n",
    "        tnr = tn/(negative)\n",
    "    metrics['TrueNegativeRate'] = tnr\n",
    "    \n",
    "    # Precision = True Positive/(True Positive + False Positive)\n",
    "    # How many positives classified by the algorithm are really positives? (fraction)\n",
    "    # Precision value closer to 1 is better. closer to 0 is worse\n",
    "    if tp == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = tp/(tp + fp)\n",
    "    metrics['Precision'] = precision\n",
    "    \n",
    "    # Accuracy = (True Positive + True Negative)/(total positive + total negative)\n",
    "    # How many positives and negatives were correctly classified? (fraction)\n",
    "    # Accuracy value closer to 1 is better. closer to 0 is worse\n",
    "    accuracy = (tp + tn)/(positive + negative)\n",
    "    metrics['Accuracy'] = accuracy\n",
    "    \n",
    "    # False Positive Rate (FPR, False Alarm) = False Positive/(total negative)\n",
    "    # How many negatives were mis-classified as positives (fraction)\n",
    "    # False Positive Rate value closer to 0 is better. closer to 1 is worse\n",
    "    if fp == 0:\n",
    "        fpr = 0\n",
    "    else:\n",
    "        fpr = fp/(negative)\n",
    "    metrics['FalsePositiveRate'] = fpr\n",
    "    \n",
    "    # False Negative Rate (FNR, Misses) = False Negative/(total Positive)\n",
    "    # How many positives were mis-classified as negative (fraction)\n",
    "    # False Negative Rate value closer to 0 is better. closer to 1 is worse\n",
    "    fnr = fn/(positive)\n",
    "    metrics['FalseNegativeRate'] = fnr\n",
    "    \n",
    "    # F1 Score = harmonic mean of Precision and Recall\n",
    "    # F1 Score closer to 1 is better. Closer to 0 is worse.\n",
    "    if precision == 0 or recall == 0:\n",
    "        f1 = 0\n",
    "    else:        \n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "    metrics['F1'] = f1\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: \n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "    #    print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember: Class 1 is the class of interest (customers that require follow up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(df['Actual'], df['Predicted'],labels=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Followup','Normal'],\n",
    "                      title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Among the positive customers, the model predicted 106 of them correctly and mis-classified 53 as normal\n",
    "* Among the negative customers, the model predicted 928 of them correctly and mis-classified 13 as positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Followup','Normal'],\n",
    "                      title='Confusion Matrix - Fraction', normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When viewed as a percentage, the model missed 33% of positive customers and 1% are false alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [binary_classifier_metrics(df['Actual'], df['Predicted'])]\n",
    "df_metrics=pd.DataFrame.from_dict(metrics)\n",
    "df_metrics.index = ['Model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Counts')\n",
    "print(df_metrics[['TruePositive',\n",
    "                  'FalseNegative',\n",
    "                  'FalsePositive',\n",
    "                  'TrueNegative',]].round(2))\n",
    "print()\n",
    "print('Fractions')\n",
    "print(df_metrics[['Recall',\n",
    "                  'FalseNegativeRate',\n",
    "                  'FalsePositiveRate',\n",
    "                  'TrueNegativeRate',]].round(2))\n",
    "print()\n",
    "\n",
    "print(df_metrics[['Precision',\n",
    "                  'Accuracy',\n",
    "                  'F1']].round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(\n",
    "    df['Actual'],\n",
    "    df['Predicted'],\n",
    "    labels=[1,0],\n",
    "    target_names=['Followup','Normal']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_score = roc_auc_score(df['Actual'],df['Predicted_Proba_Class1'])\n",
    "print('AUC Score: {0:.3f}'.format(roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('Actual').reset_index(drop=True)\n",
    "df.head()\n",
    "df['Actual'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = df[df['Actual']==0]\n",
    "followup = df[df['Actual']==1]\n",
    "plt.figure()\n",
    "plt.scatter(df.index,df['Actual'],label='actual')\n",
    "plt.scatter(normal.index,normal['Predicted_Proba_Class1'],label='normal')\n",
    "plt.scatter(followup.index,followup['Predicted_Proba_Class1'],label='followup')\n",
    "plt.plot([df.index.min(),df.index.max()],[0.5,0.5],color='r')\n",
    "plt.xlim(left=0)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Predicted Probability')\n",
    "plt.title('Followup / Normal')\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The model has done a decent job with negative customers\n",
    "* However, there are lots of positive customers with a probability < 0.5 and the model is classifying them as negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One approach to improve Recall is to lower the threshold so we can identify more positive customers\n",
    "* Let's say every customer that needs to be followed up has some cost associated with it (say ```$10```)\n",
    "* We can ignore true negatives as no action is needed with them\n",
    "* False negatives can be highly costly as it's a missed opportuinty to address some customer concern or an issue raised by the customer\n",
    "* Let's say that cost is ```$50 ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the optimal cutoff\n",
    "\n",
    "```txt\n",
    "$50 * FN(C) + $0 * TN(C) + $10 * FP(C) + $10 * TP(C)\n",
    "```\n",
    "\n",
    "FN(C) means that the false negative percentage is a function of the cutoff, C, and similar for TN, FP, and TP.  We need to find the cutoff, C, where the result of the expression is smallest.\n",
    "\n",
    "A straightforward way to do this, is to simply run a simulation over a large number of possible cutoffs.  We test 100 possible values in the for loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=df['Actual'],columns=np.where(df['Predicted_Proba_Class1'] > .5, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = np.arange(0.1, .9, 0.01)\n",
    "costs = []\n",
    "for c in cutoffs:\n",
    "    costs.append(np.sum(np.sum(np.array([[0, 10], [50, 10]]) * \n",
    "                               pd.crosstab(index=df['Actual'],columns=np.where(df['Predicted_Proba_Class1'] > c, 1, 0)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = np.array(costs)\n",
    "plt.plot(cutoffs, costs)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Cutoff')\n",
    "plt.show()\n",
    "\n",
    "print('Cost is minimized near a cutoff of:', cutoffs[np.argmin(costs)], 'for a cost of:', np.min(costs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = df[df['Actual']==0]\n",
    "followup = df[df['Actual']==1]\n",
    "plt.figure()\n",
    "plt.scatter(df.index,df['Actual'],label='actual')\n",
    "plt.scatter(normal.index,normal['Predicted_Proba_Class1'],label='normal')\n",
    "plt.scatter(followup.index,followup['Predicted_Proba_Class1'],label='followup')\n",
    "plt.plot([df.index.min(),df.index.max()+50],[cutoffs[np.argmin(costs)],cutoffs[np.argmin(costs)]],color='r',linewidth=3)\n",
    "plt.xlim(left=0)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Predicted Probability')\n",
    "plt.title('Followup / Normal')\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(df['Actual'], np.where(df['Predicted_Proba_Class1'] > cutoffs[np.argmin(costs)], 1, 0),labels=[1,0])\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Followup','Normal'],\n",
    "                      title='Confusion Matrix at {0:0.2f}'.format(cutoffs[np.argmin(costs)]), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(df['Actual'], np.where(df['Predicted_Proba_Class1'] > .5, 1, 0),labels=[1,0])\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Followup','Normal'],\n",
    "                      title='Confusion Matrix at 0.5', normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you compare the two confusion matrices, we can now identify 79% of the positives compared to the 67% when cutoff was 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For a classifier, finding the optimal cutoff based on business cost is a great approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
